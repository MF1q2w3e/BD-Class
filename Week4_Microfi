##########################################
##### WEEK 4 HOMEWORK MICROFINANCE #######
##########################################
## microfinance network 
## data from BANERJEE, CHANDRASEKHAR, DUFLO, JACKSON 2012



##### LOAD DATA

## data on 8622 households
hh <- read.csv("microfi_households.csv", row.names="hh")



###### SET UP CODE FROM THE PROFESSOR ########################################
##### SOME GRAPHING OF THE "NETWORK STUFF" TO HELP ORIENT OURSELVES 


## We'll kick off with a bunch of network stuff.
## This will be covered in more detail in lecture 6.
## get igraph off of CRAN if you don't have it
## install.packages("igraph")
## this is a tool for network analysis
## (see http://igraph.sourceforge.net/)

library(igraph) # igraph manipulates graph objects
edges <- read.table("microfi_edges.txt", colClasses="character")

## edges holds connections between the household ids
hhnet <- graph.edgelist(as.matrix(edges)) # I believe this creates a graph object of our network -KS 
hhnet <- as.undirected(hhnet) # two-way connections. # I believe this ensures the relationships between nodes are 2-day -KS

## igraph is all about plotting.  
V(hhnet) ## our 8000+ household vertices
#This gives you a list of how each node connects with each other node. 
# I think using the "V()" function translates the information into a digestible format
# If you use "head()" you get a bunch of sparse matrix gobbledygood that says the same thing in a less intuitive way

## Each vertex (node) has some attributes, and we can add more.
V(hhnet)$village <- as.character(hh[V(hhnet),'village']) # This adds the village information to our hhnet network


## we'll color them by village membership
vilcol <- rainbow(nlevels(hh$village))
names(vilcol) <- levels(hh$village)
V(hhnet)$color = vilcol[V(hhnet)$village]


## drop HH labels from plot
V(hhnet)$label=NA


# graph plots try to force distances proportional to connectivity
# imagine nodes connected by elastic bands that you are pulling apart
# The graphs can take a very long time, but I've found
# edge.curved=FALSE speeds things up a lot.  Not sure why.

## we'll use induced.subgraph and plot a couple villages 
village1 <- induced.subgraph(hhnet, v=which(V(hhnet)$village=="1"))
village33 <- induced.subgraph(hhnet, v=which(V(hhnet)$village=="33"))

# vertex.size=3 is small.  default is 15
plot(village1, vertex.size=3, edge.curved=FALSE)
plot(village33, vertex.size=3, edge.curved=FALSE)



####################################### SET UP CODE FROM PROFESSOR

library(gamlr)

## match id's; I call these 'zebras' because they are like crosswalks
zebra <- match(rownames(hh), V(hhnet)$name)

## calculate the `degree' of each hh: 
##  number of commerce/friend/family connections
degree <- degree(hhnet)[zebra]
names(degree) <- rownames(hh)
degree[is.na(degree)] <- 0 # unconnected houses, not in our graph

## if you run a full glm, it takes forever and is an overfit mess
# > summary(full <- glm(loan ~ degree + .^2, data=hh, family="binomial"))
# Warning messages:
# 1: glm.fit: algorithm did not converge 
# 2: glm.fit: fitted probabilities numerically 0 or 1 occurred 



#### HOMEWORK QUESTIONS BEGIN ##################################

### QUESTION 1: MOIZ
### [1]. I'd transform degree to create our treatment variable d.
### What would you do and why?





### QUESTION 2: KATERINA
### [2]. Build a model to predict d from x, our controls.
### Comment on how tight the fit is, and what that
### implies for estimation of a treatment effect.


## Understanding the data
summary(degree) # the range of the data is from 0 to 90, with a mean at 9
plot(degree) # notice that there are a bunch of outliers skewing things a bit
# We can't transform with log because some of the numbers are 0s
plot(degree)
hist(degree)# skewed to the left


# Rejected transformations:
#d<- degree^2
#d <- (degree-mean(degree))/sd(degree) 


d <- log(1+degree) # This is our test variable
y <- hh$loan # Our Y will be whether or not they take out a loan
x <- cBind(hh$village, hh$religion, hh$roof, 
           hh$rooms, hh$electricity, hh$ownership, hh$leader) # our X will be everything else
# Note you HAVE to use cBind on your x variables, or gamlr gets mad at you



# Okay, enough playin'. Let's get our gamlr on! 
library(gamlr)

# Run the lasso on everything blindly:
naive <- gamlr(cBind(d,x),y,
               family="binomial", lambda.min.ratio=0.001)
coef(naive)["d",] # effect is AICc selected <0
plot(naive)

# Run the lasso without including degree 
strawman <- gamlr(x,y,
                  family="binomial", lambda.min.ratio=0.001)


#### Now let's try the double lasso

# ROUND 1: REGRESS X ON D
#first, let's take d out of x:

# Regress x on d 
treat <- gamlr(x,d, lambda.min.ratio=0.001)

# Describe the treat regression
plot(treat)
log(which.min(AICc(treat))) #AICc

# Now, grab the predicted treatment
dhat <- predict(treat, x, type="response") 

## Let's look at how much signal is "left over"
plot(dhat,d,bty="n") 
cor(drop(dhat),d)^2 #IS R2
## Note: IS R2 is what governs how much independent signal you have for estimating 



### QUESTION 3: KATERINA
### [3]. Use predictions from [2] in an estimator for effect of d on loan.


# ROUND 2: RERUN THE LASSO INCLUDING dhat
# re-run lasso, with this (2nd column) included unpenalized
causal <- gamlr(cBind(d,dhat,x),y,free=2, family="binomial")
log(coef(causal)["d",])
plot(causal)
log(which.min(AICc(causal)))

# type="response" is redundant here (gaussian), 
# but you'd want it if d was binary
yhat <- predict(causal,cBind(d,dhat,x), type="response") #I think the type should be binary or binomial, but it won't parse...
plot(yhat,dhat)
plot(yhat,y)
cor(drop(yhat),y)^2 #IS R2



## Look more closely at our final model, "causal"

# Calcuate AICc
causalAIC <- AICc(causal)
log(causal$lambda[which.min(causalAIC)])
plot(causalAIC)

# Crossvalidation
causal.cv <- cv.gamlr(cBind(d,dhat,x),y,nfold=10, free=2, family="binomial", lambda.min.ratio=0.001)
log(causal.cv$lambda.min)
log(causal.cv$lambda.1se)

naive.cv <- cv.gamlr(cBind(d,x),y,nfold=10, family="binomial", lambda.min.ratio=0.001)
log(naive.cv$lambda.min)
log(naive.cv$lambda.1se)


straw.cv <- cv.gamlr(cBind(x),y,nfold=10, family="binomial", lambda.min.ratio=0.001)
log(straw.cv$lambda.min)
log(straw.cv$lambda.1se)


# Compare Coefficients
coef(naive)["d",] # effect is AICc selected <0
coef(treat)["d",] # effect is AICc selected <0
coef(causal)["d",] # effect is AICc selected <0

#Compare AICcs
log(which.min(AICc(strawman))) #D not included
log(which.min(AICc(naive))) #Naive
log(which.min(AICc(treat))) #X on D
log(which.min(AICc(causal))) #X+D on Y

#Compare IS R2
cor(drop(dhat),d)^2 #IS R2 dhat
cor(drop(yhat),y)^2 #IS R2 yhat


#Gimmie the topline:
log(which.min(AICc(causal))) #AICc
causal.cv <- cv.gamlr(cBind(d,dhat,x),y,nfold=10, free=2, family="binomial", lambda.min.ratio=0.001)
log(causal.cv$lambda.min) #CV.min
coef(causal)["d",] # coeff for d










### QUESTION 4: NAREEN
### [4]. Compare the results from [3] to those from a straight (naive) lasso
### for loan on d and x. Explain why they are similar or different.





### QUESTION 5: MATT
### [5]. Bootstrap your estimator from [3] and describe the uncertainty.

## Repurposing the below:
# first get a rowcount on x:
n <- nrow(x)
## then create and empty gamma vector:
gamb <- c() 
for(b in 1:1000){  # <-- iterate through:
  ## create a matrix of resampled indices
  ib <- sample(1:n, n, replace=TRUE)
  ## create the resampled data
  xb <- x[ib,]
  db <- d[ib]
  yb <- y[ib]
  ## re-run the treatment regression
  treatb <- gamlr(xb,db,lambda.min.ratio=1e-3)
  dhatb <- predict(treatb, xb, type="response")  
  fitb <- gamlr(cBind(db,dhatb,xb),yb,free=2, family="binomial")
  # assign to gamma vector
  gamb <- c(gamb,coef(fitb)["db",])
  print(b)
}

# Run a summary on gamb:
print("Summary of gamb:")
print(summary(gamb)) 

print("StdDev of gamb:")
print(sd(gamb)) 

print("95% Confidence interval of gamb:")
print(c("+2sd: ", mean(gamb)+(2*sd(gamb)) ))
print(c("-2sd: ", mean(gamb)-(2*sd(gamb)) ))
print("d coefficient from causal model:")
print(coef(causal)["d",])

# Then output a histogram
hist(gamb, main="Dist. of Bootstrapped \nEstimator Coefs.", xlab="Gammas", col=rgb(0,0,1,0.5))
abline(v=mean(gamb),col="green", lwd=3)
abline(v=coef(causal)["d",],col="red", lwd=3)
legend("topright", bty="n", c("coef(causal)[d,]", "mean(gamb)","95% Conf Int."), col = c("red","green",rgb(0,1,0,.2)), lwd = 2, cex = 0.75)
polygon(x=c(rep(mean(gamb)-(2*sd(gamb)),2),rep(mean(gamb)+(2*sd(gamb)),2)), y=c(0,200,200,0), col=rgb(0,1,0,.2), border=NA)

### QUESTION 6: MOIZ
### [+]. Can you think of how you'd design an experiment
### to estimate the treatment effect of network degree?



